{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# US Role"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use Gephi to plot the contagion network centering on the U.S. By utilizing the known CTG index and BGGM data, we calculate the nodes and edges for the network recognizable by Gephi."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# Function to find top 3 correlated countries\n",
    "def find_top_countries(df_path):\n",
    "    df = pd.read_csv(df_path, index_col=0)\n",
    "    us_row = df.loc['US']\n",
    "    sorted_us_row = us_row.apply(abs).sort_values(ascending=False)\n",
    "    top_3_countries = sorted_us_row.drop('US').head(3)\n",
    "    return top_3_countries.index.tolist(), us_row[top_3_countries.index]\n",
    "\n",
    "\n",
    "# Function to compute average weights and node sizes\n",
    "def compute_total_impact_average_weights_and_node_sizes(df, start_date, end_date):\n",
    "    df_filtered = df[(df['date'] >= pd.Timestamp(start_date)) & (df['date'] < pd.Timestamp(end_date))]\n",
    "    avg_weights = df_filtered.mean(numeric_only=True)\n",
    "    node_impact = {node: 0 for node in ['b', 'f', 's', 'g']}\n",
    "    for col in df.columns[1:]:\n",
    "        start_node = col[0]\n",
    "        weight = avg_weights[col]\n",
    "        node_impact[start_node] += weight\n",
    "    total_impact = sum(node_impact.values())\n",
    "    node_sizes = {node: impact  for node, impact in node_impact.items()} # / total_impact*5000\n",
    "    edge_weights = {col: abs(avg_weights[col]/ max(abs(avg_weights.min()), avg_weights.max()))  for col in df.columns[1:]} #以最大值为标准，范围是0~1\n",
    "    return total_impact, node_sizes, edge_weights\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "\n",
    "critical_date = ['2013-01-01','2014-10-30','2015-12-17','2019-08-01','2021-11-04','2023-06-01']\n",
    "country_code_to_name = {\n",
    "    'AUS': 'Australia',\n",
    "    'CAN': 'Canada',\n",
    "    'FRA': 'France',\n",
    "    'GER': 'Germany',\n",
    "    'JPN': 'Japan',\n",
    "    'UK': 'UK',\n",
    "    'US': 'US',\n",
    "    'ARG': 'Argentina',\n",
    "    'BRA': 'Brazil',\n",
    "    'CHN': 'China',\n",
    "    'IND': 'India',\n",
    "    'KOR': 'Korea',\n",
    "    'MEX': 'Mexico',\n",
    "    'RUS': 'Russia',\n",
    "    'SIN': 'Singapore',\n",
    "    'THA': 'Thailand'\n",
    "}\n",
    "label_mapping = {'b': 'bond', 'f': 'forex', 's': 'stock', 'g': 'gold'}\n",
    "label_mapping2 = {'bond':'b', 'forex':'f', 'stock':'s', 'gold':'g'}\n",
    "\n",
    "composite_index={}\n",
    "\n",
    "for k in range(len(critical_date) - 1):\n",
    "    node_sizes_combined = {}\n",
    "    edge_weights_combined = {}\n",
    "    start_date, end_date = critical_date[k], critical_date[k + 1]\n",
    "\n",
    "    country_label = 'US'\n",
    "    df = pd.read_csv('Data//all_ctg_index_value//ctg_allvalue_US.csv')\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    total_impact, node_sizes, edge_weights = compute_total_impact_average_weights_and_node_sizes(df, start_date, end_date)\n",
    "    composite_index['US']=total_impact\n",
    "\n",
    "    node_sizes_combined.update({f'{country_label}_{label_mapping[k]}': v for k, v in node_sizes.items()})\n",
    "    edge_weights_combined.update({(f'{country_label}_{label_mapping[k[0]]}', f'{country_label}_{label_mapping[k[1]]}'): v for k, v in edge_weights.items()})\n",
    "\n",
    "\n",
    "    for market_type in ['bond', 'forex', 'stock', 'gold']:\n",
    "\n",
    "        # Load top 3 countries and their correlation for the current market type\n",
    "        _, top_3_countries_original_values = find_top_countries(f'Data\\\\BGGM\\\\pcorr_{market_type}\\\\pcorr_{k+1}.csv')\n",
    "        filtered_series = top_3_countries_original_values[top_3_countries_original_values != 0]\n",
    "        #top_3_countries_original_values.drop(top_3_countries_original_values[top_3_countries_original_values == 0].index, inplace=True)\n",
    "        # Load data and compute node sizes and edge weights for each country and the current time range\n",
    "        data_files = [f'Data//all_ctg_index_value//ctg_allvalue_{country_code_to_name[country]}.csv' for country in filtered_series.index]\n",
    "\n",
    "\n",
    "        for i, file in enumerate(data_files):\n",
    "            country_label = filtered_series.index[i]\n",
    "\n",
    "            df = pd.read_csv(file)\n",
    "            df.drop(df.columns[0], axis=1, inplace=True)\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "            total_impact,node_sizes, edge_weights = compute_total_impact_average_weights_and_node_sizes(df, start_date, end_date)\n",
    "            composite_index[country_label]=total_impact\n",
    "            #node_sizes_combined[f'{country_label}_{market_type}']=node_sizes[label_mapping2[market_type]]\n",
    "            #edge_weights_combined[(f'US_{market_type}',f'{country_label}_{market_type}')]=filtered_series.iloc[i]\n",
    "            node_sizes_combined.update({f'{country_label}_{label_mapping[k]}': v for k, v in node_sizes.items()})\n",
    "            edge_weights_combined[(f'US_{market_type}',f'{country_label}_{market_type}')]=filtered_series.iloc[i]\n",
    "            edge_weights_combined.update({(f'{country_label}_{label_mapping[k[0]]}', f'{country_label}_{label_mapping[k[1]]}'): v for k, v in edge_weights.items()})\n",
    "\n",
    "    # Convert edge_weights_combined to a 3-column dictionary format (Source, Target, Weight)\n",
    "    three_col_dict = {'Source': [], 'Target': [], 'Weight': []}\n",
    "\n",
    "    for (source, target), weight in edge_weights_combined.items():\n",
    "        three_col_dict['Source'].append(source)\n",
    "        three_col_dict['Target'].append(target)\n",
    "        three_col_dict['Weight'].append(weight)\n",
    "\n",
    "    edges_df = pd.DataFrame(three_col_dict)\n",
    "    nodes_df = pd.DataFrame(node_sizes_combined.items(),columns=['Id','Size'])\n",
    "\n",
    "    # Define the color mapping for nodes\n",
    "    node_colors = {'bond': 'red', 'forex': 'blue', 'stock': 'purple', 'gold': 'gold'}\n",
    "    color_mapping = {\n",
    "    'red': [255, 1, 1],  # 红色的RGB值\n",
    "    'blue': [1, 1, 255],  # 蓝色的RGB值\n",
    "    'purple': [128, 1, 128],  # 紫色的RGB值\n",
    "    'gold': [245, 208, 1],\n",
    "    #'cyan': [1, 255, 255],  # 青色的RGB值\n",
    "    'green': [1, 128, 1],  # 绿色的RGB值\n",
    "    'orange': [255, 165, 1]  # 橙色的RGB值\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "    # Add color information to nodes\n",
    "    nodes_df['Color'] = nodes_df['Id'].apply(lambda x: color_mapping[node_colors[x.split('_')[-1]]])\n",
    "    nodes_df['Label'] = nodes_df['Id'].apply(lambda id: id.split('_')[1][0].upper())\n",
    "\n",
    "\n",
    "    # Process the edges\n",
    "    # Create a dictionary to hold the raw edges with their weights and sources\n",
    "    raw_edges = {}\n",
    "    for _, row in edges_df.iterrows():\n",
    "        edge_key = tuple(sorted([row['Source'], row['Target']]))\n",
    "        raw_edges.setdefault(edge_key, []).append((row['Weight'], row['Source'], row['Target']))\n",
    "\n",
    "    # Calculate the weight difference for bidirectional edges and determine edge color\n",
    "    processed_edges = []\n",
    "    for edge, weights_sources_targets in raw_edges.items():\n",
    "        if len(weights_sources_targets) == 2:\n",
    "            # Sort by weight to find the larger weight and corresponding source and target\n",
    "            sorted_weights_sources_targets = sorted(weights_sources_targets, key=lambda x: x[0], reverse=True)\n",
    "            weight_diff = sorted_weights_sources_targets[0][0] - sorted_weights_sources_targets[1][0]\n",
    "            source = sorted_weights_sources_targets[0][1]\n",
    "            target = sorted_weights_sources_targets[0][2]\n",
    "            edge_color = nodes_df[nodes_df['Id'] == source]['Color'].values[0]\n",
    "\n",
    "            #edge_color = nodes_df[nodes_df['Id'] == source]['Color'].values[0] if edge_type == 'Directed' else ('0,128,0' if weight_diff >= 0 else '255,165,0')\n",
    "\n",
    "\n",
    "            edge_type = 'Directed'\n",
    "        else:\n",
    "            weight_diff = abs(weights_sources_targets[0][0])\n",
    "            source = weights_sources_targets[0][1]\n",
    "            target = weights_sources_targets[0][2]\n",
    "            edge_color = color_mapping['green'] if weights_sources_targets[0][0] >= 0 else color_mapping['orange']\n",
    "            edge_type = 'Undirected'\n",
    "\n",
    "        processed_edges.append({'Source': source, 'Target': target, 'Type': edge_type, 'Weight': weight_diff, 'Color': edge_color})\n",
    "\n",
    "    # Create a dataframe from the processed edges\n",
    "    processed_edges_df = pd.DataFrame(processed_edges)\n",
    "    composite_index_df = pd.DataFrame(composite_index.items(),columns=['Country','Composite Index'])\n",
    "\n",
    "    # Save the processed dataframes to new CSV files\n",
    "    nodes_df.to_csv(f'DataAnalysis//US Role//Gephi Data//phase{k+1}_nodes.csv', index=False)\n",
    "    processed_edges_df.to_csv(f'DataAnalysis//US Role//Gephi Data//phase{k+1}_edges.csv', index=False)\n",
    "    #composite_index_df.to_csv(f'Data//Gephi//phase{k+1}_composite_index.csv', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
